<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>逻辑回归</title>
    <url>/2025/11/25/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<p>简单的逻辑回归和样例ヾ(<em>´∀ ˋ</em>)ﾉ<br><span id="more"></span>  <!-- 就加这行！前面的内容（标题+前置知识）首页不显示，只显文章标题 --><br><del>这篇文章是本蒟蒻第一个博客，各处表述难免欠妥，希望各位佬指出</del></p>
<pre><code>前置知识：线性回归，简单概率论，一点点机器学习基础
</code></pre><h1 id="什么是逻辑回归"><a href="#什么是逻辑回归" class="headerlink" title="什么是逻辑回归"></a>什么是逻辑回归</h1><p>逻辑回归 <code>logistic regression</code> 是一种经典分类算法，通常用于二分类。逻辑回归是一种对数线性模型(从下面的公式也可以看出来)  </p>
<h1 id="逻辑回归基础"><a href="#逻辑回归基础" class="headerlink" title="逻辑回归基础"></a>逻辑回归基础</h1><p>先来了解一下逻辑斯谛分布, 设 <code>X</code> 为连续随机变量, 逻辑斯谛分布的累积分布函数为：</p>
<script type="math/tex; mode=display">F(x) = P(X \leq x) = \frac{1}{1 + e^{-\frac{x - \mu}{\gamma}}}</script><p>对应的概率密度函数（CDF 的导数）为：</p>
<script type="math/tex; mode=display">f(x) = F'(x) = \frac{e^{-\frac{x - \mu}{\gamma}}}{\gamma \cdot \left(1 + e^{-\frac{x - \mu}{\gamma}}\right)^2}</script><p>式中 $\mu$ 为位置参数,  $\gamma$ 为形状参数  </p>
<p><del>好复杂看不懂</del><br>先看看简单的。<br>当逻辑斯谛分布的位置参数 $\mu$、形状参数 $\gamma$ 时，其累积分布函数（CDF）就是 sigmoid 函数（激活函数）：</p>
<script type="math/tex; mode=display">sigmoid(x) = \frac{1}{1 + e^{-x}}</script><p>作用：将($-\infty, +\infty$) 映射到 (0, 1)</p>
<p>再结合线性回归，就可以做到逻辑回归最简单的应用: 二分类  </p>
<p>就是<strong>将线性回归的输出作为逻辑回归的输入</strong><br>具体一点，就是将线性回归的值，带入上面的激活函数，映射到 (0-1) 后，再根据阈值分类<br>选定阈值（0-1）后大于阈值为正样本， 小于为负样本</p>
<script type="math/tex; mode=display">h(x)=sigmoid(w^T·x+b)</script><h2 id="下面介绍逻辑回归损失函数"><a href="#下面介绍逻辑回归损失函数" class="headerlink" title="下面介绍逻辑回归损失函数"></a>下面介绍逻辑回归损失函数</h2><script type="math/tex; mode=display">Loss(L) = -y_i \log p - (1 - y_i) \log(1 - p)</script><p>式中，$y_i$ 为逻辑回归的预估的类别，A类为1，B类为0，即正样本只看前半部分，负样本只看后半部分。<br> $p$ 为上面 h(x) 的返回值，即概率。<br>损失函数的工作原理：每个样本有A，B两个类别，<strong>真实类别对应的位置，概率值越大越好</strong>  </p>
<p>下面具体讲讲概率分布  </p>
<h3 id="对于一个样本"><a href="#对于一个样本" class="headerlink" title="对于一个样本"></a>对于一个样本</h3><p>假设有0，1两个类别，显然有：</p>
<script type="math/tex; mode=display">
L= 
\begin{cases}
p & y = 1 \quad (\text{样本为正类}) \\
1-p & y = 0 \quad (\text{样本为负类})
\end{cases}</script><p>等价于</p>
<script type="math/tex; mode=display">L=p^y(1-p)^{1-y}</script><h3 id="推广到n个样本"><a href="#推广到n个样本" class="headerlink" title="推广到n个样本"></a>推广到n个样本</h3><script type="math/tex; mode=display">P=\displaystyle \prod_{i=1}^{n}{p^{y_i}(1-p)^{1-{y_i}}}</script><p>这个就是所有样本都预测正确的概率。</p>
<p>问题转换为：当上述联合概率取最大时，估计 $w$ 和 $b$ ，就变成了极大似然估计。</p>
<h3 id="极大似然函数转对数似然函数"><a href="#极大似然函数转对数似然函数" class="headerlink" title="极大似然函数转对数似然函数"></a>极大似然函数转对数似然函数</h3><p>对上述式子取对数，有:</p>
<script type="math/tex; mode=display">H(L)=\sum_{i=1}^n y_i \log p + (1 - y_i) \log(1 - p)</script><p>因为是损失函数，所以应该取最小值，只需加个负号。</p>
<script type="math/tex; mode=display">Loss(L)=-\sum_{i=1}^n y_i \log p + (1 - y_i) \log(1 - p)</script><p>也就得到了之前是式子。<br>通过这个转换，将最小化损失函数的任务交给了极大似然函数，所以逻辑回归本质上没有新的算法。  </p>
<p>总结即是：逻辑回归损失函数= - 极大似然估计函数  </p>
<h1 id="实战训练——癌症预测"><a href="#实战训练——癌症预测" class="headerlink" title="实战训练——癌症预测"></a>实战训练——癌症预测</h1><p>逻辑回归的包<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br></pre></td></tr></table></figure></p>
<p>简单介绍一下api<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">LogisticRegression(solver=<span class="string">&#x27;liblinear&#x27;</span>,penalty=<span class="string">&#x27;l2&#x27;</span>,C=<span class="number">1.0</span>)</span><br></pre></td></tr></table></figure><br>其中 <code>liblinear</code> 在数据集较小是比较快，对大数据集也可以换成 <code>sag</code> or <code>saga</code><br> <code>penalty</code> 表示正则化种类，l1或者l2<br>C 表示正则化力度<br>默认将数量少的类别作为正例  </p>
<p>下面是完整代码，只是换成了 <code>LogisticRegression</code> ,其他和别的方法一样</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">d1</span>():</span><br><span class="line">    df = pd.read_csv(<span class="string">&quot;breast-cancer-wisconsin.data&quot;</span>, sep=<span class="string">&quot;,&quot;</span>, header=<span class="literal">None</span>)</span><br><span class="line">    <span class="comment"># df.info()</span></span><br><span class="line">    df=df.replace(to_replace=<span class="string">&#x27;?&#x27;</span>,value=np.NaN)</span><br><span class="line">    df=df.dropna()</span><br><span class="line">    df.info()</span><br><span class="line">    x=df.iloc[:,<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">    y=df.iloc[:,-<span class="number">1</span>]</span><br><span class="line">    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=<span class="number">0.2</span>,random_state=<span class="number">22</span>)</span><br><span class="line">    transfer=StandardScaler()</span><br><span class="line">    x_train=transfer.fit_transform(x_train)</span><br><span class="line">    x_test=transfer.transform(x_test)</span><br><span class="line">    model=LogisticRegression()</span><br><span class="line">    model.fit(x_train,y_train)</span><br><span class="line">    y_pred=model.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;score: <span class="subst">&#123;model.score(x_test,y_test)&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    d1()</span><br></pre></td></tr></table></figure>
<h1 id="模型的评估"><a href="#模型的评估" class="headerlink" title="模型的评估"></a>模型的评估</h1><p>像上面那样，仅使用 ‘准确率’ 进行估计，事实上是不精确，甚至是不可取的  </p>
<p>比如：1000 人中仅 10 人患癌（正例占 1%，健康人占 99%，极度不平衡）若模型 “躺平” 全预测 “健康”，准确率 = 990/1000=99%（看似优秀），但漏诊了所有 10 名患者（相当于最坏情况下根本没测出来）。<br>而如果说继续提升这个准确率，确实可以提高测出来的比例，但是这在精度上的要求将是巨大的，现实情况样本容量巨大，实现是不可能的。  </p>
<p>在正负样例容量差距较大时，需要进行进一步的评估，下面介绍一些方法  </p>
<h2 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center"></th>
<th style="text-align:center">正例（预测）</th>
<th style="text-align:center">反例（预测）</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">正例（真实）</td>
<td style="text-align:center">真正例（TP）</td>
<td style="text-align:center">伪反例（FN）</td>
</tr>
<tr>
<td style="text-align:center">反例（真实）</td>
<td style="text-align:center">伪正例（FP）</td>
<td style="text-align:center">真反例（TN）</td>
</tr>
</tbody>
</table>
</div>
<p>其中，T/F表示真/假，P/N表示正/负（样例）<br>比如伪反例，就是本来应该是正例，被错误预测为反例，即假的反例 <code>False Negative</code>  </p>
<p>下面的评估需要用到混淆矩阵  </p>
<ul>
<li>精确率 <code>precision</code> <script type="math/tex; mode=display">P= \frac{TP}{TP + FP}</script></li>
<li>召回率（查全率） <code>recall</code><script type="math/tex; mode=display">P= \frac{TP}{TP + FN}</script></li>
<li><code>F1 score</code> (用于综合上面两个)  <script type="math/tex; mode=display">P= \frac{2·precision·recall}{precision + recall}</script></li>
</ul>
<p>从各公式就可以明显看出来这三个值的含义了，再简单解释一下，精确率就是预测出来的正例确实是正例的概率，查全率就是有多少正例被预测出来了。这两个指标当然越大越好，F1值即综合上面两个指标，也就是可以用F1值作为模型好坏的评价，<strong>F1值越大模型越好</strong>。  </p>
<p>代码也很简单，先加个包<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, f1_score,confusion_matrix</span><br></pre></td></tr></table></figure><br>其他的和上面的代码一样，得分改成F1值<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;F1 Score (pos_label=4): <span class="subst">&#123;f1_score(y_test, y_pred, pos_label=<span class="number">4</span>):<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure><br>当然这里也可以不加 <code>pos_label</code> ，默认样本少的为正例<br>也可以打印出混淆矩阵来看看<br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cm = confusion_matrix(y_test, y_pred)</span><br><span class="line">cm_df = pd.DataFrame(</span><br><span class="line">    cm,</span><br><span class="line">    index=[<span class="string">&quot;真实_良性(2)&quot;</span>, <span class="string">&quot;真实_恶性(4)&quot;</span>],</span><br><span class="line">    columns=[<span class="string">&quot;预测_良性(2)&quot;</span>, <span class="string">&quot;预测_恶性(4)&quot;</span>]</span><br><span class="line">)</span><br><span class="line"><span class="built_in">print</span>(cm_df)</span><br></pre></td></tr></table></figure><br>至此，就可以得到更加精准的模型评估  </p>
<h2 id="ROC曲线和AUC指标"><a href="#ROC曲线和AUC指标" class="headerlink" title="ROC曲线和AUC指标"></a>ROC曲线和AUC指标</h2><p>这两个就是进一步拓展了，这里简单讲一点  </p>
<p>ROC <code>Receiver Operating Characteristic Curve</code> 曲线可以就是上面混淆矩阵的可视化  </p>
<ul>
<li><p>横轴（FPR）：假阳性率 = 假阳性样本数/(假阳性+真阴性样本数) → 模型误将负样本判为正样本的比例（反映“误诊”风险）  </p>
</li>
<li><p>纵轴（TPR）：真阳性率 = 真阳性样本数/(真阳性+假阴性样本数) → 模型正确识别正样本的比例（即召回率，反映“漏诊”风险）  </p>
</li>
</ul>
<p>同一数据集下，性能更优的模型ROC曲线会更靠近左上角（TPR高且FPR低） </p>
<p>AUC <code>Area Under the ROC Curve</code> 是ROC曲线下方的面积，取值范围为0~1：</p>
<p>核心意义：代表模型在随机抽取一个正样本和一个负样本时，正样本被预测为正类的概率大于负样本的概率（即模型区分正/负样本的能力, 也可以作为模型评分）<br>使用AUC可以直接比较多个模型的整体性能  </p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>逻辑回归看着带“回归”二字，其实是做分类的，尤其适合分二类（比如判断是不是癌症）。</p>
<p>核心思路很简单：先拿线性回归算出一个结果，再用sigmoid函数把这个结果变成0到1之间的概率，定个阈值（比如0.5），概率超阈值就是正例，反之就是负例。</p>
<p>实战里要注意：</p>
<ul>
<li>先给特征做标准化（不然模型不准）；</li>
<li>遇到像癌症预测这种“少数人患病、多数人健康”的情况，别只看准确率（容易骗人），用混淆矩阵、F1分数或者AUC这些指标才靠谱，能避免漏诊、误诊。</li>
</ul>
<p>它的好处就是模型简单、训练快，还能看懂每个特征的影响（比如哪个指标对判断癌症更重要），是新手入门机器学习的好选择</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title>随机森林</title>
    <url>/2025/11/27/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97/</url>
    <content><![CDATA[<pre><code>前置知识：决策树
</code></pre><h1 id="一点背景"><a href="#一点背景" class="headerlink" title="一点背景"></a>一点背景</h1><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><h3 id="什么是集成学习"><a href="#什么是集成学习" class="headerlink" title="什么是集成学习"></a>什么是集成学习</h3><p>普通模型将数据训练，得到结果。集成学习是一种机器学习的思想，将多个模型组合成一个精度更高的模型（强学习器）。被组合的小模型成为弱学习器（基学习器）<br>通过多个模型集成来提升整体精度，而不是专门培养一个模型  </p>
<ul>
<li>一个单一强大模型：能力很强，但有过拟合风险，而且抗风险能力弱</li>
<li>多个小模型组合：也可以有很强的整体能力，但是减少了过拟合风险，抗风险能力也提高了  <h3 id="集成学习的分类"><a href="#集成学习的分类" class="headerlink" title="集成学习的分类"></a>集成学习的分类</h3>Bagging（并行）和Boosting（串行）<br>Bagging是将原始数据进行多个流程训练综合得到结果，多个过程可以并行（比如在学校一学期同时学数学和英语）<br>Boosting是将原始数据经过第一个过程处理后，输入第二个过程继续…（比如数学先学习1+1，再学微积分）  </li>
</ul>
<p>本文探讨的随机森林就是Bagging中的一个<br>Boosting主要有：Adaboost，GBDT，XGBoost等  </p>
<h2 id="决策树回顾"><a href="#决策树回顾" class="headerlink" title="决策树回顾"></a>决策树回顾</h2><p>决策树是一颗二叉树，是监督学习中经典的分类/回归模型，它通过模拟人类“分而治之”的决策过程，将复杂问题拆解为一系列简单判断。简单来说，各结点就是各判断条件，根据判断结果的真伪细分到下面的子树继续判断。  </p>
<p>决策树有多种，ID3，C4.5，CART等，随机森林主要用到CART决策树  </p>
<p>决策树有剪枝操作，比如限制最大深度，结点继续划分的条件等，这些在随机森林api中会再看到  </p>
<p>从这两个的名字也可以猜出来，其实随机森林的 ‘森林’ 就是由一个个决策树构成的。  </p>
<h1 id="Bagging思想"><a href="#Bagging思想" class="headerlink" title="Bagging思想"></a>Bagging思想</h1><p>随机森林是一种基于Bagging思想的集成学习方法，以决策树作为弱学习器。<br>具体过程：</p>
<ul>
<li>将训练集通过随机抽样（有放回）分为几个小训练集</li>
<li>各个小训练集再各自随机挑选一些特征进行训练  </li>
<li>构建出n个决策树，然后这些决策树进行平权投票，选出最终结果  </li>
</ul>
<p>这里注意是要有放回的随机抽样，而且抽样比例最好不要太少。如果不抽样，那同样的训练集训练出来的决策树都是一样的。有放回保证了各决策树有交叉相连的关系，因此每棵树不会有较大偏差，既有交集又有差异，更好发挥投票的效果。  </p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>随机森林</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
</search>
